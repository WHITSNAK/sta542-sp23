\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, margin=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[font=scriptsize]{caption}
\usepackage{subcaption}
\usepackage{xcolor}

\newtheorem{lemma}{Lemma}
\newcommand{\indep}{\perp \!\!\! \perp}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    texcl=true,
    mathescape=true,
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\graphicspath{ {.} }
\captionsetup{justification=raggedright, singlelinecheck=false}

\author{Ryan Tang}
\title{STA 542 HW 2}
\date{March 2nd 2023}

\begin{document}
\maketitle

\section{Ex 1}
Suppose we know the underlying truth $Y = X^2 + Z$. Both $X$ and $Z$ are standard normal $N(0,1)$ and independent of each other $X \indep Z$. Then the best predictor $E[Y|X] = X^2$. On the other hand, the best linear predictor is simply the expected mean of $Y$ because of the following.
\begin{align*}
    \text{Cov}(Y, X) &= \text{Cov}(X^2 + Z, X) \\
        &= \text{Cov}(X^2, X) + \text{Cov}(Z, X) \\
        &= E[X^3] - E[X]E[X^2] + 0 \\
        &= 0 \\
    E[Y \in \{\mathcal{M}: Y = a + bX\}|X] &= E[Y] = E[X^2] + E[Z] = 1
\end{align*}

\section{Ex 2}
\paragraph{(a)}
Here we plot $f(t) = A \cos(2\pi \times 5t + \alpha)$, where $A = \pi, \aplha = 1$ and between $0 < t \leq 5$ at 120Hz. The resulting $f_{478} = 2.324$.
\begin{align*}
    f_t = \pi \cos(2\pi \times 5t + 1)
\end{align*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{plot1.png}
  \captionsetup{justification=centering}
  \caption{The ground truth cosine wave}
\end{figure*}

\paragraph{(b)}
Now, we adds a $MA(2|\theta=(0.4, 1.6))$ noise to the ground truth with each individual $w_t \thicksim_{iid} N(0,v=1)$. Hence, we have $Y_t = f_t + X_t$. See Figure \ref{fig:truth_plus_noise}.

\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{plot2.png}
  \captionsetup{justification=centering}
  \caption{The ground truth cosine wave plus MA(2) noises, $Y_t = f_t + X_t$.}
  \label{fig:truth_plus_noise}
\end{figure*}

\paragraph{(c)}
Suppose we know $X_t$ follows MA(2) process and would like to estimate the coefficients directly from the observations $Y_t$. We get the following coefficients $\hat{\theta}=(0.76, 0.2523), \hat{v}=5.6$. Obviously, it is way off from the ground truth $\theta = (0.4 1.6), v=1$.

\paragraph{(d)}
We can also consider the ARIMA model, using the finite differences to remove and trend, $Z_t = (1-B)^d Y_t$. It is under the assumption that we can recover the stationary series after the finite differences. However, we know the underlying series; we can't arrive at anything stationary. For example, the first-order difference of $f_t$ gives us a mixture of two cosine waves at a different phase.
\begin{align*}
    (1-B)f_t &= f_t - f_{t-1} \\
        &= \pi [cos(2\pi \times 5t + 1) - cos(2\pi \times 5t - 4)]
\end{align*}

Furthermore, we can see taking the finite differences creates an artificially high amplitude which certainly doesn't resemble the $MA(2|\theta=(0.4, 1.6))$ model for $X_t$.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{plot3.png}
  \captionsetup{justification=centering}
  \caption{Finite differences of $Y_t$ up to the 3rd order.}
  \label{fig:truth_plus_noise}
\end{figure*}

\paragraph{(e)}
One reasonable approach is to use SARIMA to model the data because we can use Periodogram to investigate the amplitude on different frequencies. We see a spike at 5Hz in Figure \ref{fig:periodogram} which signals the periodicity at every 24th sample at the 120Hz sampling rate or every $0.2t$. Hence, we can assume the following model.
\begin{align*}
    (1 - B^{0.2})Y_t &= X_t - X_{t-0.2} \\
        &\thicksim ARIMA(0,0,2) \times (0,1,1)_{0.2}
\end{align*}
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.5\textwidth]{plot4.png}
  \captionsetup{justification=centering}
  \caption{Periodogram of $Y_t$}
  \label{fig:periodogram}
\end{figure*}

With the model defined, I have plotted the prediction results and 200 forward forecasts against the observation in Figure \ref{fig:sarima_forecast}.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{plot5.png}
  \captionsetup{justification=centering}
  \caption{$ARIMA(0,0,2) \times (0,1,1)_{0.2}$ in-sample and out-sample forecasts.}
  \label{fig:sarima_forecast}
\end{figure*}

Lastly, I check the residual against with Gaussian assumption. It looks fine.
\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{plot6.png}
  \captionsetup{justification=centering}
  \caption{SARIMA residual diagnostic under normality assumption.}
  \label{fig:sarima_diagnostic}
\end{figure*}

\paragraph{(f)}

\paragraph{(g)}
\paragraph{(h)}

\end{document}